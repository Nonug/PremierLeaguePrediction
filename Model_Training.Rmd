---
title: "Model Training"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(pacman)
p_load(
    "caret", "skimr", "RANN", "randomForest", "fastAdaboost",
    "gbm", "xgboost", "caretEnsemble", "C50", "earth",
    DataExplorer, nnet, broom, glmnet, MLmetrics, e1071,
    doParallel
)
```

## Import dataset
```{r}
train_df_timestamped <- read.csv("./data/training_data.csv")
train_df <- subset(train_df_timestamped,
    select = -c(Season, Date, Date_interval)
)

test_df <- read.csv("./data/testing_data.csv")
test_df <- subset(test_df, select = -c(Season, Date, Date_interval))
```

```{r}
# convert cat variables to factor
chr_cols <- c("FTR", "HomeTeam", "AwayTeam", "Referee")

# This step prevents errors when fitting test data to models
# if the levels of factors aren't the same.
for (col in chr_cols) {
    # Create a levels attribute with the unique values
    # from attribute1 from both test and train.
    level <- unique(c(train_df[, col], test_df[, col]))
    # Create a factor on train and test attribute1 with all
    # the levels found above.
    train_df[, col] <- factor(train_df[, col], levels = level)
    test_df[, col] <- factor(test_df[, col], levels = level)
}

# level <- unique(c(train_df$FTR, test_df$FTR))

# Store X and Y for later use.
y <- train_df$FTR
x <- subset(train_df, select = -c(FTR))
x_num <- subset(x, select = -c(HomeTeam, AwayTeam, Referee))
```

Plot distribution of A/D/H in training data
```{r}
ydist_plot <- ggplot(train_df, aes(FTR, fill = FTR)) +
    geom_bar() +
    geom_text(stat = "count", aes(
        label = round(
            ..count.. / sum(..count..), 3
        ),
        vjust = -1
    )) +
    ggtitle("Distribution of A/D/H in training data")

ydist_plot
ggsave(ydist_plot,
    filename = "./img/train_y_dist.png",
    height = 6, width = 6
)
```

Create DataExplorer report 
```{r}
# create_report(x, output_file = "Xtrain_report.html")
```
### One-Hot Encoding
```{r}
# Converting a categorical variable to
# as many binary variables as here are categories.
dummies_model <- dummyVars(FTR ~ ., data = train_df)

# Create the dummy variables using predict.
# The Y variable (Purchase) will not be present in trainData_mat.
trainData_mat <- predict(dummies_model, newdata = train_df)

# Convert to dataframe
train_encoded <- data.frame(trainData_mat)

# See the structure of the new dataset
str(train_encoded)
colnames(train_encoded)
```



Feature plots (check ./img)
```{r}
# Defining a function to save lattice plots
lattice_save <- function(plot, name) {
    setwd("./img/")
    png(file = paste(name, ".png", sep = ""))
    plot(plot)
    dev.off() # Saving the file
    setwd("../")
}
# Generate a feature boxplot.
feature_boxplot <- featurePlot(
    x = x_num,
    y = y,
    plot = "box",
    strip = strip.custom(par.strip.text = list(cex = .7)),
    scales = list(
        x = list(relation = "free"),
        y = list(relation = "free")
    )
)

feature_density <- featurePlot(
    x = x_num,
    y = y,
    plot = "density",
    strip = strip.custom(par.strip.text = list(cex = .7)),
    scales = list(
        x = list(relation = "free"),
        y = list(relation = "free")
    )
)

lattice_save(feature_boxplot, "training_feature_boxplot")
lattice_save(feature_density, "training_feature_density")
```

## Multinomial Logistic Regression model with Cross Validation
### Using glmnet and caret
```{r}
set.seed(3612)

trControl <- trainControl(
    method = "repeatedcv",
    number = 5, # 5-fold CV
    repeats = 5, # repeat 5-fold CV 5 times
    p = 0.75, # the training percentage, default = 75%
    search = "grid", # grid search, Default
    savePredictions = "final", # saves predictions for optimal tuning parameter
    classProbs = T, # should class probabilities be returned
    summaryFunction = multiClassSummary, # results summary function
    verboseIter = T,
)
# Elastic net multinom
multinom_CV <- train(
    form = FTR ~ .,
    data = train_df,
    trControl = trControl,
    method = "glmnet",
    family = "multinomial"
)

multinom_CV
fitted <- predict(multinom_CV)
multinom_CV_plot <- plot(multinom_CV,
    main = "Model Accuracies with caret + glmnet Multinom"
)
lattice_save(multinom_CV_plot, "Model Accuracies with caret + glmnet Multinom")
confusionMatrix(reference = y, data = fitted, mode = "everything")
```

### Using nnet

Below we use the multinom function from the nnet package to estimate a 
multinomial logistic regression model using neural nets 
(out-of-syllabus? Drop this maybe)

```{r}
# Training the multinomial model
# DummyVar not required.
multinom_model <- multinom(FTR ~ ., data = train_df, seed = 3612)
# Checking the model
summary(multinom_model)

# Convert coef to prob, get first 6 predictions.
head(round(fitted(multinom_model), 2))
```

#### Predicting & Validating

```{r}
# Predicting the values for train dataset
FTRPredicted <- predict(multinom_model, newdata = train_df, "class")
# Building classification table
tab <- table(y, FTRPredicted)
# Calculating accuracy - sum of diagonal elements divided by total obs
round((sum(diag(tab)) / sum(tab)) * 100, 2)
```



## Support Vector Machine

### Stock SVM using radial kernal
Accuracy = 0.6083
```{r}
svmfit <- svm(FTR ~ ., data = train_df)
summary(svmfit)
svm_pred <- predict(svmfit, train_df)
confusionMatrix(reference = train_df$FTR, data = svm_pred, mode = "everything")
```

#### Test data results
```{r}
# svm_pred_test <- predict(svmfit, test_df)

# confusionMatrix(reference = test_df$FTR, data = svm_pred_test, mode = "everything")
```

### xgboost
```{r}
set.seed(3612)

trControl <- trainControl(
    method = "cv",
    number = 5, # 5-fold CV
    # repeats = 5, # repeat 5-fold CV 5 times
    p = 0.75, # the training percentage, default = 75%
    search = "grid", # grid search, Default
    savePredictions = "final", # saves predictions for optimal tuning parameter
    classProbs = T, # should class probabilities be returned
    summaryFunction = multiClassSummary, # results summary function
    verboseIter = T,
)


# cl <- makePSOCKcluster(3)
# registerDoParallel(cl)

## All subsequent models are then run in parallel
xgboost_mod <- train(
    x = train_encoded,
    y = y,
    trControl = trControl,
    method = "xgbDART",
    # tuneLength = 5,
)




xgboost_mod
fitted_xgb <- predict(xgboost_mod, train_df)
multinom_CV_plot <- plot(xgboost_mod,
    main = "Model Accuracies with caret + xgbDART"
)
lattice_save(xgboost_mod, "Model Accuracies with caret + glmnet Multinom")
confusionMatrix(reference = y, data = fitted_xgb, mode = "everything")

## When you are done:
# stopCluster(cl)
```
