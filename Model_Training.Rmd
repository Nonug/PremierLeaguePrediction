---
title: "Model Training"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(pacman)
p_load(
    "caret", "skimr", "RANN", "randomForest", "fastAdaboost",
    "gbm", "xgboost", "caretEnsemble", "C50", "earth",
    DataExplorer, nnet, broom, glmnet
)
```

## Import dataset
```{r}
train_df_timestamped <- read.csv("./data/training_data.csv")
train_df <- subset(train_df_timestamped, select = -c(Season, Date))
# test_df <- read.csv("./data/testing_data.csv")

# convert cat variables to factor
chr_cols <- c("FTR", "HomeTeam", "AwayTeam", "Referee")
train_df[, chr_cols] <- lapply(train_df[, chr_cols], factor)

# Store X and Y for later use.
y <- train_df$FTR
x <- subset(train_df, select = -c(FTR))
x_num <- subset(x, select = -c(HomeTeam, AwayTeam, Referee))
```
### One-Hot Encoding
```{r}
# Converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(FTR ~ ., data = train_df)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mat <- predict(dummies_model, newdata = train_df)

# Convert to dataframe
train_encoded <- data.frame(trainData_mat)

# See the structure of the new dataset
str(train_encoded)
colnames(train_encoded)
```


```{r}
# create_report(x, output_file = "Xtrain_report.html")
```

Feature plots (check ./img)
```{r}
# Defining a function to save lattice plots
lattice_save <- function(plot, name) {
    setwd("./img/")
    png(file = paste(name, ".png", sep = ""))
    print(plot)
    dev.off() # Saving the file
    setwd("../")
}
# Generate a feature boxplot.
feature_boxplot <- featurePlot(
    x = x_num,
    y = y,
    plot = "box",
    strip = strip.custom(par.strip.text = list(cex = .7)),
    scales = list(
        x = list(relation = "free"),
        y = list(relation = "free")
    )
)

feature_density <- featurePlot(
    x = x_num,
    y = y,
    plot = "density",
    strip = strip.custom(par.strip.text = list(cex = .7)),
    scales = list(
        x = list(relation = "free"),
        y = list(relation = "free")
    )
)

lattice_save(feature_boxplot, "training_feature_boxplot")
lattice_save(feature_density, "training_feature_density")
```

## Multinomial Logistic Regression model with Cross Validation

### Using glmnet and caret
```{r}
set.seed(3612)

trControl <- trainControl(
    method = "repeatedcv",
    number = 5,
    repeats = 5,
    search = "random",
    verboseIter = TRUE
)
# trControl <- trainControl(method = "cv", number = 5)

multinom_CV <- train(
    form = FTR ~ .,
    data = train_df,
    trControl = trControl,
    method = "glmnet",
    family = "multinomial"
)

multinom_CV
fitted <- predict(multinom_CV)
plot(multinom_CV, main = "Model Accuracies with caret Multinom")

confusionMatrix(reference = y, data = fitted, mode = "everything")
```

### Using nnet
Below we use the multinom function from the nnet package to estimate a 
multinomial logistic regression model. 

There are other functions in other R packages capable of multinomial regression. 

We chose the multinom function because it does not require the data to be reshaped 
(as the mlogit package does)
```{r}
# Training the multinomial model
# DummyVar not required.
multinom_model <- multinom(FTR ~ ., data = train_df, seed = 3612)
# Checking the model
summary(multinom_model)

# Convert coef to prob, get first 6 predictions.
head(round(fitted(multinom_model), 2))
```

#### Predicting & Validating

```{r}
# Predicting the values for train dataset
FTRPredicted <- predict(multinom_model, newdata = train_df, "class")
# Building classification table
tab <- table(y, FTRPredicted)
# Calculating accuracy - sum of diagonal elements divided by total obs
round((sum(diag(tab)) / sum(tab)) * 100, 2)
```



